{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO812DCaRLXWmC43WarArFG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Moksha-nagraj/Marvel_tasks_lvl2/blob/main/Copy_of_ID3_algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18dnw1Dp6ac0",
        "outputId": "83fe6c0d-2dca-4f74-d681-7ee5ba4a14b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Info gain for Outlook: 0.24674981977443933\n",
            "Info gain for Temperature: 0.02922256565895487\n",
            "Info gain for Humidity: 0.15183550136234159\n",
            "Info gain for Wind: 0.04812703040826949\n",
            "Most informative feature: Outlook\n",
            "Generated sub-tree for Outlook: {'Sunny': '?', 'Overcast': 'Yes', 'Rain': '?'}\n",
            "Info gain for Outlook: 0.0\n",
            "Info gain for Temperature: 0.5709505944546686\n",
            "Info gain for Humidity: 0.9709505944546686\n",
            "Info gain for Wind: 0.01997309402197489\n",
            "Most informative feature: Humidity\n",
            "Generated sub-tree for Humidity: {'High': 'No', 'Normal': 'Yes'}\n",
            "Info gain for Outlook: 0.0\n",
            "Info gain for Temperature: 0.01997309402197489\n",
            "Info gain for Humidity: 0.01997309402197489\n",
            "Info gain for Wind: 0.9709505944546686\n",
            "Most informative feature: Wind\n",
            "Generated sub-tree for Wind: {'Weak': 'Yes', 'Strong': 'No'}\n",
            "Generated tree: {'Outlook': {'Sunny': {'Humidity': {'High': 'No', 'Normal': 'Yes'}}, 'Overcast': 'Yes', 'Rain': {'Wind': {'Weak': 'Yes', 'Strong': 'No'}}}}\n",
            "Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Loading the dataset\n",
        "train_data_m = pd.read_csv(\"/content/PlayTennis.csv\")\n",
        "\n",
        "def calc_total_entropy(train_data, label, class_list):\n",
        "    total_row = train_data.shape[0]  # total size of the dataset\n",
        "    total_entr = 0\n",
        "\n",
        "    for c in class_list:  # for each class in the label\n",
        "        total_class_count = train_data[train_data[label] == c].shape[0]  # number of the class\n",
        "        if total_class_count == 0:\n",
        "            continue\n",
        "        total_class_entr = - (total_class_count / total_row) * np.log2(total_class_count / total_row)  # entropy of the class\n",
        "        total_entr += total_class_entr  # adding the class entropy to the total entropy of the dataset\n",
        "\n",
        "    return total_entr\n",
        "\n",
        "def calc_entropy(feature_value_data, label, class_list):\n",
        "    class_count = feature_value_data.shape[0]\n",
        "    entropy = 0\n",
        "\n",
        "    for c in class_list:\n",
        "        label_class_count = feature_value_data[feature_value_data[label] == c].shape[0]  # row count of class c\n",
        "        if label_class_count == 0:\n",
        "            continue\n",
        "        probability_class = label_class_count / class_count  # probability of the class\n",
        "        entropy_class = - probability_class * np.log2(probability_class)  # entropy\n",
        "        entropy += entropy_class\n",
        "    return entropy\n",
        "\n",
        "def calc_info_gain(feature_name, train_data, label, class_list):\n",
        "    feature_value_list = train_data[feature_name].unique()  # unique values of the feature\n",
        "    total_row = train_data.shape[0]\n",
        "    feature_info = 0.0\n",
        "\n",
        "    for feature_value in feature_value_list:\n",
        "        feature_value_data = train_data[train_data[feature_name] == feature_value]  # filtering rows with that feature_value\n",
        "        feature_value_count = feature_value_data.shape[0]\n",
        "        feature_value_entropy = calc_entropy(feature_value_data, label, class_list)  # calculating entropy for the feature value\n",
        "        feature_value_probability = feature_value_count / total_row\n",
        "        feature_info += feature_value_probability * feature_value_entropy  # calculating information of the feature value\n",
        "\n",
        "    total_entropy = calc_total_entropy(train_data, label, class_list)\n",
        "    info_gain = total_entropy - feature_info\n",
        "    print(f\"Info gain for {feature_name}: {info_gain}\")\n",
        "    return info_gain\n",
        "\n",
        "def find_most_informative_feature(train_data, label, class_list):\n",
        "    feature_list = train_data.columns.drop(label)  # finding the feature names in the dataset\n",
        "    max_info_gain = -1\n",
        "    max_info_feature = None\n",
        "\n",
        "    for feature in feature_list:  # for each feature in the dataset\n",
        "        feature_info_gain = calc_info_gain(feature, train_data, label, class_list)\n",
        "        if max_info_gain < feature_info_gain:  # selecting feature name with highest information gain\n",
        "            max_info_gain = feature_info_gain\n",
        "            max_info_feature = feature\n",
        "\n",
        "    print(f\"Most informative feature: {max_info_feature}\")\n",
        "    return max_info_feature\n",
        "\n",
        "def generate_sub_tree(feature_name, train_data, label, class_list):\n",
        "    feature_value_count_dict = train_data[feature_name].value_counts(sort=False)  # dictionary of the count of unique feature value\n",
        "    tree = {}  # sub tree or node\n",
        "\n",
        "    for feature_value, count in feature_value_count_dict.items():\n",
        "        feature_value_data = train_data[train_data[feature_name] == feature_value]  # dataset with only feature_name = feature_value\n",
        "\n",
        "        assigned_to_node = False  # flag for tracking feature_value is pure class or not\n",
        "        for c in class_list:  # for each class\n",
        "            class_count = feature_value_data[feature_value_data[label] == c].shape[0]  # count of class c\n",
        "\n",
        "            if class_count == count:  # count of (feature_value = count) of class (pure class)\n",
        "                tree[feature_value] = c  # adding node to the tree\n",
        "                assigned_to_node = True\n",
        "        if not assigned_to_node:  # not pure class\n",
        "            tree[feature_value] = \"?\"  # as feature_value is not a pure class, it should be expanded further,\n",
        "                                      # so the branch is marking with ?\n",
        "\n",
        "    print(f\"Generated sub-tree for {feature_name}: {tree}\")\n",
        "    return tree\n",
        "\n",
        "def make_tree(root, prev_feature_value, train_data, label, class_list):\n",
        "    if train_data.shape[0] != 0:  # if dataset is not empty after updating\n",
        "        max_info_feature = find_most_informative_feature(train_data, label, class_list)  # most informative feature\n",
        "        if max_info_feature is None:\n",
        "            return\n",
        "        tree = generate_sub_tree(max_info_feature, train_data, label, class_list)  # getting tree node and updated dataset\n",
        "\n",
        "        if prev_feature_value is not None:  # add to intermediate node of the tree\n",
        "            root[prev_feature_value] = {max_info_feature: tree}\n",
        "        else:  # add to root of the tree\n",
        "            root[max_info_feature] = tree\n",
        "\n",
        "        for feature_value, sub_tree in tree.items():\n",
        "            if sub_tree == \"?\":  # if it is expandable\n",
        "                feature_value_data = train_data[train_data[max_info_feature] == feature_value]  # using the updated dataset\n",
        "                make_tree(tree, feature_value, feature_value_data, label, class_list)  # recursive call with updated dataset\n",
        "\n",
        "def id3(train_data_m, label):\n",
        "    train_data = train_data_m.copy()  # getting a copy of the dataset\n",
        "    tree = {}  # tree which will be updated\n",
        "    class_list = train_data[label].unique()  # getting unique classes of the label\n",
        "    make_tree(tree, None, train_data, label, class_list)  # start calling recursion\n",
        "    print(\"Generated tree:\", tree)\n",
        "    return tree\n",
        "\n",
        "tree = id3(train_data_m, 'Play Tennis')\n",
        "\n",
        "def predict(tree, instance):\n",
        "    if not isinstance(tree, dict):  # if it is leaf node\n",
        "        return tree  # return the value\n",
        "    elif not tree:  # check if the dictionary is empty\n",
        "        print(\"Empty tree encountered!\")\n",
        "        return None\n",
        "    else:\n",
        "        root_node = next(iter(tree))  # getting first key/feature name of the dictionary\n",
        "        feature_value = instance[root_node]  # value of the feature\n",
        "        if feature_value in tree[root_node]:  # checking the feature value in current tree node\n",
        "            return predict(tree[root_node][feature_value], instance)  # go to next feature\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "def evaluate(tree, test_data_m, label):\n",
        "    correct_predict = 0\n",
        "    wrong_predict = 0\n",
        "    for index, row in test_data_m.iterrows():  # for each row in the dataset\n",
        "        result = predict(tree, test_data_m.iloc[index])  # predict the row\n",
        "        if result == test_data_m[label].iloc[index]:  # predicted value and expected value is same or not\n",
        "            correct_predict += 1  # increase correct count\n",
        "        else:\n",
        "            wrong_predict += 1  # increase incorrect count\n",
        "    accuracy = correct_predict / (correct_predict + wrong_predict)  # calculating accuracy\n",
        "    return accuracy\n",
        "\n",
        "test_data_m = pd.read_csv(\"/content/PlayTennis.csv\")  # importing test dataset into dataframe\n",
        "\n",
        "accuracy = evaluate(tree, test_data_m, 'Play Tennis')  # evaluating the test dataset\n",
        "print(f\"Accuracy: {accuracy}\")\n"
      ]
    }
  ]
}